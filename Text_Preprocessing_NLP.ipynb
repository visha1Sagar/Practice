{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/visha1Sagar/Practice/blob/main/Text_Preprocessing_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qa8x0dIl2JES"
      },
      "source": [
        "# **Text Preprocessing for NLP**\n",
        "\n",
        "\n",
        "In this lab we will learn how to preprocess raw text data and make it suitable for further processing by DL models. We will perform this processing in two ways.\n",
        "1. Using NLTK Library\n",
        "2. Using Tensorflow\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W12-tRwM0tx7"
      },
      "source": [
        "#Using NLTK\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmYtHCUg00Qx"
      },
      "source": [
        "Import NLTK library and download relevant modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwqbPqbOn0dO"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "random.seed(92)\n",
        "\n",
        "# Load the TensorBoard notebook extension\n",
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VBcykaE27VX"
      },
      "source": [
        "Let's define a corpus of raw text. For this example the following text is taken from wikipedia page on NLP (https://en.wikipedia.org/wiki/Natural_language_processing).\n",
        "You will notice that the text contains words, numbers, punctuations, and other special characters. This is RAW form of text. For different NLP tasks, it is required to preprocess this data to make it more structures and workable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q1Hk9thOoo9z"
      },
      "outputs": [],
      "source": [
        "corpus = str([\"\"\"In 2003, word n-gram model, at the time the best statistical algorithm, was overperformed by a multi-layer perceptron (with a single hidden layer and context length of several words trained on up to 14 million of words with a CPU cluster in language modelling) by Yoshua Bengio with co-authors.[8]\n",
        "In 2010, Tomáš Mikolov (then a PhD student at Brno University of Technology) with co-authors applied a simple recurrent neural network with a single hidden layer to language modelling,[9] and in the following years he went on to develop Word2vec.\n",
        "In the 2010s, representation learning and deep neural network-style (featuring many hidden layers) machine learning methods became widespread in natural language processing.\n",
        "That popularity was due partly to a flurry of results showing that such techniques[10][11] can achieve state-of-the-art results in many natural language tasks, e.g., in language modeling[12] and parsing.[13][14]\n",
        "This is increasingly important in medicine and healthcare, where NLP helps analyze notes and text in electronic health records that would otherwise be inaccessible for study when seeking to improve care[15] or protect patient privacy.[16]\"\"\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nsqvjw614EkK"
      },
      "source": [
        "**Tokenization**\n",
        "Tokenization means dividing a piece of raw text into semantically and syntactically meaningful units, called tokens.\n",
        "\n",
        "(word tokens).\n",
        "\n",
        "These tokens can be\n",
        "\n",
        "1.   setence-level (sentence tokens)\n",
        "2.   word-level\n",
        "\n",
        "The following code passes the raw text (corpus) to the sent_tokenize() method which converts the text into sentences. Each sentence is then further tokenized into smaller units using word_tokenize() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUj2pksvo4ul"
      },
      "outputs": [],
      "source": [
        "#NLTK recognises each word seperated by white-space, number, and punctuations as distinct tokens.\n",
        "token_list = []\n",
        "sentences = nltk.sent_tokenize(str(corpus))\n",
        "for sentence in sentences:\n",
        "  words = nltk.word_tokenize(sentence)\n",
        "  token_list.extend(words) #list appending\n",
        "  #tagged_words = nltk.pos_tag(words)\n",
        "  #named_entities = nltk.ne_chunk(tagged_words)\n",
        "\n",
        "# pos_tag() method and ne_chunk() methods perform Part-Of-Speech tagging and Named-Entity Recognition, respectively.\n",
        "# The POS tagging is a task in which each word is tagged with a part-of-speech category. For example, the word 'horse' is tagged as a noun and 'walk' is tagged as a verb.\n",
        "# A Named-Entity is a word (or a set of words) that represent name of a person, place, organisation, unit of measurement, etc.\n",
        "\n",
        "print(token_list)\n",
        "print ('\\nLength of word_list:', len(token_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hmVcmxNoCIQz"
      },
      "source": [
        "Another way to tokenize a piece of text using NLTK is to use the split() method as shown below. (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzxM_dzpq2Iw"
      },
      "outputs": [],
      "source": [
        "word_list = corpus.split()\n",
        "print(word_list)\n",
        "print ('\\nLength of word_list:', len(word_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBslCXNHCTs6"
      },
      "source": [
        "Next, we filter the tokens and keep only those that contains alphabets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7qqLc00Zzd5I"
      },
      "outputs": [],
      "source": [
        "alphabets_only = [word for word in word_list if word.isalpha()]\n",
        "print(alphabets_only)\n",
        "print ('\\nLength of word_list:', len(alphabets_only))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CR29nqjbDUj0"
      },
      "source": [
        "**Text Normalisation** means to remove any capitalisation in the text and convert all tokens to lower case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBXX3g7l0LyI"
      },
      "outputs": [],
      "source": [
        "lower_case = [word.lower() for word in alphabets_only]\n",
        "print(lower_case)\n",
        "print ('\\nLength of word_list:', len(lower_case))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5pYZcPFDrKe"
      },
      "source": [
        "**Stopword removal** means to remove those words from the remaining tokens that appear frequently in English language but contribute very little to the overall meaning of the text. It is important to note here that stopword removal should not be applied to every NLP task since in some application these stopwords are important as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XW0Qe6nL0iQo"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords_nltk = set(stopwords.words('english'))\n",
        "print(stopwords_nltk)\n",
        "print ('\\nLength of word_list:', len(stopwords_nltk), '\\n')\n",
        "\n",
        "cleaned_words = [word for word in lower_case if word not in stopwords_nltk]\n",
        "print(cleaned_words)\n",
        "print ('\\nLength of word_list:', len(cleaned_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJA-Xu6s0pCE"
      },
      "source": [
        "#Using Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfQtkxeIENaK"
      },
      "source": [
        "This concludes the first part of this lab which deals with preprocessing raw text using NLTK. Now Let's use tensorflow to perform similar preprocessing (note the difference in the implementation of both libraries) and build a complete example of sentiment analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mQ25HF5r6ib"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "sentences = [\n",
        "    'i love my dog',\n",
        "    'I, love my cat',\n",
        "    'You love my dog!'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100)\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EglaZfzNwtns"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import datetime\n",
        "\n",
        "sentences = [\n",
        "    'I love my dog',\n",
        "    'I love my cat',\n",
        "    'You love my dog!',\n",
        "    'Do you think my dog is amazing?'\n",
        "]\n",
        "\n",
        "tokenizer = Tokenizer(num_words = 100, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "sequences = tokenizer.texts_to_sequences(sentences)\n",
        "#pad the sequences with the same length\n",
        "#make all sequences in a batch fit a given standard length\n",
        "padded = pad_sequences(sequences, maxlen=5)\n",
        "print(\"\\nWord Index = \" , word_index)\n",
        "print(\"\\nSequences = \" , sequences)\n",
        "print(\"\\nPadded Sequences:\")\n",
        "print(padded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VtEnSsGhw7Fd"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/learning-datasets/sarcasm.json \\\n",
        "    -O /tmp/sarcasm.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It80kX2I0Zin"
      },
      "outputs": [],
      "source": [
        "with open(\"/tmp/sarcasm.json\", 'r') as f:\n",
        "    datastore = json.load(f)\n",
        "\n",
        "sentences = []\n",
        "labels = []\n",
        "\n",
        "for item in datastore:\n",
        "    sentences.append(item['headline'])\n",
        "    labels.append(item['is_sarcastic'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PTErLmdvBFH"
      },
      "outputs": [],
      "source": [
        "vocab_size = 10000    #same as no of words\n",
        "embedding_dim = 16\n",
        "max_length = 100\n",
        "trunc_type='post'\n",
        "padding_type='post'\n",
        "oov_tok = \"<OOV>\"\n",
        "training_size = 20000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPqjSK960d8S"
      },
      "outputs": [],
      "source": [
        "training_sentences = sentences[0:training_size]\n",
        "testing_sentences = sentences[training_size:]\n",
        "training_labels = labels[0:training_size]\n",
        "testing_labels = labels[training_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUtxvaJH0hec"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer(num_words=10000, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "training_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "training_padded = pad_sequences(training_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuXb3H750jXI"
      },
      "outputs": [],
      "source": [
        "# Need this block to get it to work with TensorFlow 2.x\n",
        "import numpy as np\n",
        "training_padded = np.array(training_padded)\n",
        "training_labels = np.array(training_labels)\n",
        "testing_padded = np.array(testing_padded)\n",
        "testing_labels = np.array(testing_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIFR8YFm0ng7"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(24, activation='relu'),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "# sgmd for two class\n",
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEzL76dc0q06"
      },
      "outputs": [],
      "source": [
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "history = model.fit(training_padded, training_labels, epochs=30 , validation_data=(testing_padded, testing_labels), verbose=2, callbacks=[tensorboard_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FECUG7ZC0zFA"
      },
      "outputs": [],
      "source": [
        "sentence = [\"granny starting to fear spiders in the garden might be real\", \"game of thrones season finale showing this sunday night\"]\n",
        "sequences = tokenizer.texts_to_sequences(sentence)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n",
        "print(model.predict(padded))\n",
        "\n",
        "# binary classification.\n",
        "# sarcastic class and Non sarcastic class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwBYkPKtDkik"
      },
      "outputs": [],
      "source": [
        "%tensorboard --logdir logs/fit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--qpISlW0t-z"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.plot(history.history['val_'+string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.legend([string, 'val_'+string])\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, \"accuracy\")\n",
        "plot_graphs(history, \"loss\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1m0JBXrPpQSO"
      },
      "source": [
        "#Language modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "zX4Kg8DUTKWO"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMzs80-IpG3S"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%204%20-%20Lesson%202%20-%20Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BOwsuGQQY9OL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pylt5qZYsWPh"
      },
      "outputs": [],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://storage.googleapis.com/learning-datasets/irish-lyrics-eof.txt \\\n",
        "    -O /tmp/irish-lyrics-eof.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRnDnCW-Z7qv"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "data = open('/tmp/irish-lyrics-eof.txt').read()\n",
        "\n",
        "corpus = data.lower().split(\"\\n\")\n",
        "\n",
        "tokenizer.fit_on_texts(corpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soPGVheskaQP"
      },
      "outputs": [],
      "source": [
        "input_sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tinput_sequences.append(n_gram_sequence)\n",
        "\n",
        "# pad sequences\n",
        "max_sequence_len = max([len(x) for x in input_sequences])\n",
        "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# create predictors and label\n",
        "xs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "\n",
        "ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJtwVB2NbOAP"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.word_index['in'])\n",
        "print(tokenizer.word_index['the'])\n",
        "print(tokenizer.word_index['town'])\n",
        "print(tokenizer.word_index['of'])\n",
        "print(tokenizer.word_index['athy'])\n",
        "print(tokenizer.word_index['one'])\n",
        "print(tokenizer.word_index['jeremy'])\n",
        "print(tokenizer.word_index['lanigan'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49Cv68JOakwv"
      },
      "outputs": [],
      "source": [
        "print(xs[6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iY-jwvfgbEF8"
      },
      "outputs": [],
      "source": [
        "print(ys[6])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtzlUMYadhKt"
      },
      "outputs": [],
      "source": [
        "print(xs[5])\n",
        "print(ys[5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4myRpB1c4Gg"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.word_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9vH8Y59ajYL"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(150)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "adam = Adam(lr=0.01)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\n",
        "#earlystop = EarlyStopping(monitor='val_loss', min_delta=0, patience=5, verbose=0, mode='auto')\n",
        "history = model.fit(xs, ys, epochs=100, verbose=1)\n",
        "#print model.summary()\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3YXGelKThoTT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poeprYK8h-c7"
      },
      "outputs": [],
      "source": [
        "plot_graphs(history, 'accuracy')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Vc6PHgxa6Hm"
      },
      "outputs": [],
      "source": [
        "seed_text = \"I've got a bad feeling about this\"\n",
        "next_words = 100\n",
        "\n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dn8VStQwQO0j"
      },
      "source": [
        "#Delieverable\n",
        "\n",
        "Write a detailed document on the learning of this lab covering the different steps involved in the preprocessing of NLP tasks and how is language modelling done in the above task. Clear explain the method and the results. Also explain how and and in what ways tensorboard helps us in analyzing model training and validation. The answer should be specific to the lab and the results obtained.\n",
        "\n",
        "Its an **individual** lab"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}